#!/usr/bin/env python3
"""
Smart Dubbing Pipeline v3
- VADë¡œ ì‚¬ëŒ ìŒì„± êµ¬ê°„ë§Œ ê°ì§€
- ë°°ê²½ìŒì•…/íš¨ê³¼ìŒ ìœ ì§€
- ìŒì„± êµ¬ê°„ì—ë§Œ ë”ë¹™ ì ìš©
"""

import os
import sys
import json
import subprocess
import tempfile
import argparse
from pathlib import Path
import torch
from openai import OpenAI
import requests
import time

# ============ ì„¤ì • ============
ELEVENLABS_API_KEY = os.getenv("ELEVENLABS_API_KEY", "sk_7b0a163f718c23222429625faebe9dabf428825ebc36d6c2")
ELEVENLABS_VOICE_ID = "pFZP5JQG7iQjIQuC4Bku"  # Lily (ì—¬ì„±) - ê¸°ë³¸ ëª©ì†Œë¦¬
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# ì‚¬ìš© ê°€ëŠ¥í•œ ElevenLabs ëª©ì†Œë¦¬ (í™”ì ë¶„ë¦¬ìš©)
ELEVENLABS_VOICES = {
    "male_1": "pNInz6obpgDQGcFmaJgB",    # Adam (ë‚¨ì„±)
    "male_2": "VR6AewLTigWG4xSOukaG",    # Arnold (ë‚¨ì„±)
    "female_1": "pFZP5JQG7iQjIQuC4Bku",  # Lily (ì—¬ì„±)
    "female_2": "21m00Tcm4TlvDq8ikWAM",  # Rachel (ì—¬ì„±)
}

# Time-stretch í—ˆìš© ë²”ìœ„
STRETCH_TOLERANCE = 0.25
MIN_STRETCH_RATIO = 1 - STRETCH_TOLERANCE
MAX_STRETCH_RATIO = 1 + STRETCH_TOLERANCE

# VAD ì„¤ì •
VAD_THRESHOLD = 0.5  # ìŒì„± ê°ì§€ ì„ê³„ê°’
MIN_SPEECH_DURATION = 0.5  # ìµœì†Œ ìŒì„± ê¸¸ì´ (ì´ˆ)
MIN_SILENCE_DURATION = 0.3  # ìµœì†Œ ë¬´ìŒ ê¸¸ì´ (ì´ˆ)


def clone_voice_elevenlabs(audio_samples: list, voice_name: str = "cloned_voice") -> str:
    """ElevenLabsì— ìŒì„± í´ë¡œë‹ ìš”ì²­ (voice_id ë°˜í™˜)"""
    print(f"[í´ë¡œë‹] ìŒì„± í´ë¡œë‹ ì¤‘: {voice_name}")
    
    url = "https://api.elevenlabs.io/v1/voices/add"
    
    headers = {
        "xi-api-key": ELEVENLABS_API_KEY
    }
    
    # íŒŒì¼ ì¤€ë¹„
    files = []
    for i, sample_path in enumerate(audio_samples):
        files.append(
            ("files", (f"sample_{i}.mp3", open(sample_path, "rb"), "audio/mpeg"))
        )
    
    data = {
        "name": voice_name,
        "description": f"Cloned voice for dubbing - {voice_name}"
    }
    
    try:
        response = requests.post(url, headers=headers, data=data, files=files)
        response.raise_for_status()
        
        result = response.json()
        voice_id = result.get("voice_id")
        print(f"[í´ë¡œë‹] ì™„ë£Œ! Voice ID: {voice_id}")
        return voice_id
        
    except Exception as e:
        print(f"[í´ë¡œë‹] ì‹¤íŒ¨: {e}")
        return None
    finally:
        # íŒŒì¼ í•¸ë“¤ ë‹«ê¸°
        for _, (_, f, _) in files:
            f.close()


def extract_voice_sample(audio_path: str, segments: list, output_path: str, 
                         target_duration: float = 30.0) -> str:
    """ìŒì„± ìƒ˜í”Œ ì¶”ì¶œ (í´ë¡œë‹ìš©, ì•½ 30ì´ˆ)"""
    print(f"[ìƒ˜í”Œ] ìŒì„± ìƒ˜í”Œ ì¶”ì¶œ ì¤‘ (ëª©í‘œ: {target_duration}ì´ˆ)")
    
    # ê¸´ ì„¸ê·¸ë¨¼íŠ¸ë“¤ ì„ íƒ (í’ˆì§ˆ ì¢‹ì€ ìƒ˜í”Œ)
    sorted_segs = sorted(segments, key=lambda x: x['duration'], reverse=True)
    
    selected = []
    total_duration = 0
    
    for seg in sorted_segs:
        if total_duration >= target_duration:
            break
        selected.append(seg)
        total_duration += seg['duration']
    
    if not selected:
        return None
    
    # ì„ íƒëœ ì„¸ê·¸ë¨¼íŠ¸ë“¤ í•©ì¹˜ê¸°
    filter_parts = []
    for i, seg in enumerate(selected):
        filter_parts.append(f"[0:a]atrim={seg['start']}:{seg['end']},asetpts=PTS-STARTPTS[a{i}]")
    
    concat_inputs = "".join([f"[a{i}]" for i in range(len(selected))])
    filter_parts.append(f"{concat_inputs}concat=n={len(selected)}:v=0:a=1[out]")
    
    filter_complex = ";".join(filter_parts)
    
    cmd = [
        "ffmpeg", "-y", "-i", audio_path,
        "-filter_complex", filter_complex,
        "-map", "[out]",
        "-c:a", "libmp3lame", "-q:a", "2",
        output_path
    ]
    
    subprocess.run(cmd, capture_output=True, check=True)
    print(f"[ìƒ˜í”Œ] ì¶”ì¶œ ì™„ë£Œ: {output_path} ({total_duration:.1f}ì´ˆ)")
    return output_path


def send_telegram_notification(message: str, chat_id: str, bot_token: str, 
                               file_path: str = None) -> bool:
    """í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì•Œë¦¼ ì „ì†¡ (íŒŒì¼ ì²¨ë¶€ ê°€ëŠ¥)"""
    try:
        if file_path and os.path.exists(file_path):
            # íŒŒì¼ê³¼ í•¨ê»˜ ì „ì†¡
            url = f"https://api.telegram.org/bot{bot_token}/sendDocument"
            with open(file_path, 'rb') as f:
                response = requests.post(url, data={
                    "chat_id": chat_id,
                    "caption": message
                }, files={"document": f})
        else:
            # í…ìŠ¤íŠ¸ë§Œ ì „ì†¡
            url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
            response = requests.post(url, json={
                "chat_id": chat_id,
                "text": message,
                "parse_mode": "HTML"
            })
        return response.status_code == 200
    except Exception as e:
        print(f"[ì•Œë¦¼] í…”ë ˆê·¸ë¨ ì „ì†¡ ì‹¤íŒ¨: {e}")
        return False


def is_youtube_url(url: str) -> bool:
    """ìœ íŠœë¸Œ URLì¸ì§€ í™•ì¸"""
    youtube_patterns = [
        'youtube.com/watch',
        'youtu.be/',
        'youtube.com/shorts/'
    ]
    return any(p in url for p in youtube_patterns)


def download_youtube(url: str, output_dir: str) -> str:
    """ìœ íŠœë¸Œ ì˜ìƒ ë‹¤ìš´ë¡œë“œ"""
    print(f"[YouTube] ë‹¤ìš´ë¡œë“œ ì¤‘: {url}")
    
    output_template = os.path.join(output_dir, "%(title).50s.%(ext)s")
    
    cmd = [
        "yt-dlp",
        "-f", "best[ext=mp4]/best",
        "-o", output_template,
        "--no-playlist",
        "--print", "after_move:filepath",
        url
    ]
    
    result = subprocess.run(cmd, capture_output=True, text=True, check=True)
    downloaded_path = result.stdout.strip().split('\n')[-1]
    
    print(f"[YouTube] ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {os.path.basename(downloaded_path)}")
    return downloaded_path


def extract_audio_wav(video_path: str, output_path: str, sample_rate: int = 16000) -> str:
    """ì˜ìƒì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ (WAV, ëª¨ë…¸)"""
    cmd = [
        "ffmpeg", "-y", "-i", video_path,
        "-vn", "-acodec", "pcm_s16le", "-ar", str(sample_rate), "-ac", "1",
        output_path
    ]
    subprocess.run(cmd, capture_output=True, check=True)
    return output_path


def detect_speech_segments(audio_path: str) -> list:
    """Silero VADë¡œ ì‚¬ëŒ ìŒì„± êµ¬ê°„ ê°ì§€"""
    print("[VAD] ìŒì„± êµ¬ê°„ ê°ì§€ ì¤‘...")
    
    import scipy.io.wavfile as wavfile
    import numpy as np
    
    # Silero VAD ëª¨ë¸ ë¡œë“œ
    model, utils = torch.hub.load(
        repo_or_dir='snakers4/silero-vad',
        model='silero_vad',
        force_reload=False,
        onnx=False,
        trust_repo=True
    )
    
    (get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = utils
    
    # scipyë¡œ ì§ì ‘ ì˜¤ë””ì˜¤ ë¡œë“œ (torchaudio ìš°íšŒ)
    sample_rate, audio_data = wavfile.read(audio_path)
    
    # int16 â†’ float32 ë³€í™˜ (-1 ~ 1 ë²”ìœ„)
    if audio_data.dtype == np.int16:
        audio_data = audio_data.astype(np.float32) / 32768.0
    elif audio_data.dtype == np.int32:
        audio_data = audio_data.astype(np.float32) / 2147483648.0
    
    # ëª¨ë…¸ë¡œ ë³€í™˜
    if len(audio_data.shape) > 1:
        audio_data = audio_data.mean(axis=1)
    
    # ë¦¬ìƒ˜í”Œë§ (16000Hz í•„ìš”)
    if sample_rate != 16000:
        from scipy import signal
        num_samples = int(len(audio_data) * 16000 / sample_rate)
        audio_data = signal.resample(audio_data, num_samples)
    
    # numpy â†’ torch tensor
    wav = torch.from_numpy(audio_data).float()
    
    # ìŒì„± êµ¬ê°„ ê°ì§€
    speech_timestamps = get_speech_timestamps(
        wav, 
        model,
        threshold=VAD_THRESHOLD,
        min_speech_duration_ms=int(MIN_SPEECH_DURATION * 1000),
        min_silence_duration_ms=int(MIN_SILENCE_DURATION * 1000),
        sampling_rate=16000
    )
    
    # ìƒ˜í”Œ â†’ ì´ˆ ë³€í™˜
    segments = []
    for ts in speech_timestamps:
        start_sec = ts['start'] / 16000
        end_sec = ts['end'] / 16000
        segments.append({
            'start': round(start_sec, 2),
            'end': round(end_sec, 2),
            'duration': round(end_sec - start_sec, 2)
        })
    
    print(f"[VAD] {len(segments)}ê°œ ìŒì„± êµ¬ê°„ ê°ì§€")
    if segments:
        print(f"      ì²« ìŒì„± ì‹œì‘: {segments[0]['start']}ì´ˆ")
    
    return segments


def extract_audio_segment(audio_path: str, start: float, end: float, output_path: str) -> str:
    """ì˜¤ë””ì˜¤ì—ì„œ íŠ¹ì • êµ¬ê°„ ì¶”ì¶œ"""
    cmd = [
        "ffmpeg", "-y", "-i", audio_path,
        "-ss", str(start), "-to", str(end),
        "-acodec", "libmp3lame", "-q:a", "2",
        output_path
    ]
    subprocess.run(cmd, capture_output=True, check=True)
    return output_path


def transcribe_audio_segment(audio_path: str, client: OpenAI) -> str:
    """Whisperë¡œ ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ í…ìŠ¤íŠ¸ ë³€í™˜"""
    with open(audio_path, "rb") as f:
        response = client.audio.transcriptions.create(
            model="whisper-1",
            file=f,
            response_format="text"
        )
    return response.strip()


def transcribe_speech_segments(audio_path: str, speech_segments: list, temp_dir: str) -> list:
    """ê° ìŒì„± êµ¬ê°„ì„ Whisperë¡œ í…ìŠ¤íŠ¸ ë³€í™˜"""
    print("[Whisper] ìŒì„± êµ¬ê°„ í…ìŠ¤íŠ¸ ë³€í™˜ ì¤‘...")
    
    client = OpenAI(api_key=OPENAI_API_KEY)
    
    results = []
    for i, seg in enumerate(speech_segments):
        print(f"  ë³€í™˜ ì¤‘: {i+1}/{len(speech_segments)}", end="\r")
        
        # êµ¬ê°„ ì¶”ì¶œ
        seg_audio_path = os.path.join(temp_dir, f"speech_seg_{i:04d}.mp3")
        extract_audio_segment(audio_path, seg['start'], seg['end'], seg_audio_path)
        
        # Whisperë¡œ í…ìŠ¤íŠ¸ ë³€í™˜
        text = transcribe_audio_segment(seg_audio_path, client)
        
        if text:  # í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°ë§Œ
            results.append({
                **seg,
                'text': text,
                'audio_path': seg_audio_path
            })
    
    print(f"\n[Whisper] {len(results)}ê°œ ì„¸ê·¸ë¨¼íŠ¸ í…ìŠ¤íŠ¸ ë³€í™˜ ì™„ë£Œ")
    return results


def translate_segments(segments: list, target_lang: str = "Korean", tone: str = "formal") -> list:
    """ì„¸ê·¸ë¨¼íŠ¸ ë²ˆì—­ (ê¸¸ì´ íŒíŠ¸ + ë§íˆ¬ í¬í•¨)"""
    print(f"[ë²ˆì—­] ë²ˆì—­ ì¤‘... (ë§íˆ¬: {tone})")
    
    client = OpenAI(api_key=OPENAI_API_KEY)
    
    # ë§íˆ¬ ì„¤ëª…
    tone_instructions = {
        "formal": "Use polite/formal Korean (ì¡´ëŒ“ë§, ~ìŠµë‹ˆë‹¤/~ìš” endings). Suitable for professional or educational content.",
        "casual": "Use casual Korean (ë°˜ë§, ~í•´/~ì•¼ endings). Suitable for friendly, informal content.",
        "narration": "Use narration style Korean (ë‚˜ë ˆì´ì…˜ì²´, ~ë‹¤/~í–ˆë‹¤ endings). Suitable for documentaries or storytelling."
    }
    
    tone_desc = tone_instructions.get(tone, tone_instructions["formal"])
    
    # ë²ˆì—­í•  ë°ì´í„° ì¤€ë¹„
    texts_with_duration = []
    for i, seg in enumerate(segments):
        texts_with_duration.append({
            "id": i,
            "text": seg["text"],
            "duration_sec": seg["duration"]
        })
    
    prompt = f"""You are a professional dubbing translator. Translate the following segments to {target_lang}.

TONE/STYLE: {tone_desc}

IMPORTANT RULES:
1. Each segment has a duration in seconds - the translation should be speakable in roughly that time
2. If the direct translation is too long, paraphrase or shorten it naturally
3. If the direct translation is too short, you can slightly expand it
4. Prioritize natural speech over literal translation
5. Keep the meaning and tone intact
6. MUST follow the specified tone/style consistently

Return a JSON array with "id" and "translation" for each segment.

Segments:
{json.dumps(texts_with_duration, ensure_ascii=False, indent=2)}

Return ONLY the JSON array, no markdown or other text."""

    response = client.chat.completions.create(
        model="gpt-5-nano",
        messages=[{"role": "user", "content": prompt}]
    )
    
    content = response.choices[0].message.content
    
    # JSON ì¶”ì¶œ
    if "```json" in content:
        content = content.split("```json")[1].split("```")[0]
    elif "```" in content:
        content = content.split("```")[1].split("```")[0]
    
    result = json.loads(content.strip())
    translation_map = {item["id"]: item["translation"] for item in result}
    
    for i, seg in enumerate(segments):
        seg["translated"] = translation_map.get(i, seg["text"])
    
    print(f"[ë²ˆì—­] ì™„ë£Œ")
    return segments


def generate_tts(text: str, output_path: str, voice_id: str = None) -> float:
    """ElevenLabs TTS ìƒì„±"""
    vid = voice_id or ELEVENLABS_VOICE_ID
    url = f"https://api.elevenlabs.io/v1/text-to-speech/{vid}"
    
    headers = {
        "xi-api-key": ELEVENLABS_API_KEY,
        "Content-Type": "application/json"
    }
    
    data = {
        "text": text,
        "model_id": "eleven_multilingual_v2",
        "voice_settings": {
            "stability": 0.5,
            "similarity_boost": 0.75
        }
    }
    
    response = requests.post(url, headers=headers, json=data)
    response.raise_for_status()
    
    with open(output_path, "wb") as f:
        f.write(response.content)
    
    return get_audio_duration(output_path)


def get_audio_duration(audio_path: str) -> float:
    """ì˜¤ë””ì˜¤ íŒŒì¼ ê¸¸ì´ ì¸¡ì •"""
    cmd = [
        "ffprobe", "-v", "error",
        "-show_entries", "format=duration",
        "-of", "default=noprint_wrappers=1:nokey=1",
        audio_path
    ]
    result = subprocess.run(cmd, capture_output=True, text=True, check=True)
    return float(result.stdout.strip())


def time_stretch_audio(input_path: str, output_path: str, ratio: float) -> str:
    """ì˜¤ë””ì˜¤ ì†ë„ ì¡°ì ˆ"""
    ratio = max(0.5, min(2.0, ratio))
    
    cmd = [
        "ffmpeg", "-y", "-i", input_path,
        "-filter:a", f"atempo={ratio}",
        "-vn", output_path
    ]
    subprocess.run(cmd, capture_output=True, check=True)
    return output_path


def process_tts_with_sync(segments: list, temp_dir: str, voice_id: str = None) -> list:
    """TTS ìƒì„± + ì‹±í¬ ì¡°ì •"""
    print("[TTS] ìŒì„± ìƒì„± ë° ì‹±í¬ ì¡°ì • ì¤‘...")
    
    processed = []
    
    for i, seg in enumerate(segments):
        print(f"  ì²˜ë¦¬ ì¤‘: {i+1}/{len(segments)}", end="\r")
        
        tts_path = os.path.join(temp_dir, f"tts_{i:04d}.mp3")
        stretched_path = os.path.join(temp_dir, f"stretched_{i:04d}.mp3")
        
        # TTS ìƒì„±
        tts_duration = generate_tts(seg["translated"], tts_path, voice_id)
        
        # íƒ€ê²Ÿ ê¸¸ì´
        target_duration = seg["duration"]
        
        # ë¹„ìœ¨ ê³„ì‚°
        stretch_ratio = target_duration / tts_duration if tts_duration > 0 else 1.0
        
        # Time-stretch ì ìš© ì—¬ë¶€ ê²°ì •
        final_path = tts_path
        
        if MIN_STRETCH_RATIO <= stretch_ratio <= MAX_STRETCH_RATIO:
            if abs(stretch_ratio - 1.0) > 0.05:
                time_stretch_audio(tts_path, stretched_path, stretch_ratio)
                final_path = stretched_path
        else:
            status = "ë¹ ë¦„" if stretch_ratio < 1 else "ëŠë¦¼"
            print(f"\n  [!] ì„¸ê·¸ë¨¼íŠ¸ {i}: ë²”ìœ„ ì´ˆê³¼ ({status}) - TTS: {tts_duration:.1f}s, íƒ€ê²Ÿ: {target_duration:.1f}s")
        
        processed.append({
            **seg,
            "tts_path": final_path,
            "tts_duration": tts_duration,
            "final_duration": get_audio_duration(final_path)
        })
    
    print(f"\n[TTS] {len(processed)}ê°œ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ ì™„ë£Œ")
    return processed


def mix_dubbed_audio(original_audio: str, segments: list, output_path: str,
                     original_volume: float = 0.15, auto_ducking: bool = True) -> str:
    """ì›ë³¸ ì˜¤ë””ì˜¤ + ë”ë¹™ ë¯¹ì‹± (ìë™ ë³¼ë¥¨ ì¡°ì ˆ ì§€ì›)"""
    print("[ë¯¹ì‹±] ì˜¤ë””ì˜¤ ë¯¹ì‹± ì¤‘...")
    
    if not segments:
        subprocess.run(["ffmpeg", "-y", "-i", original_audio, output_path], 
                      capture_output=True, check=True)
        return output_path
    
    # ì˜¤ë””ì˜¤ ê¸¸ì´ í™•ì¸
    total_duration = get_audio_duration(original_audio)
    
    if auto_ducking:
        print("[ë¯¹ì‹±] ìë™ ë³¼ë¥¨ ì¡°ì ˆ (ë”ë¹™ êµ¬ê°„ ê°ì§€)...")
        # ë”ë¹™ êµ¬ê°„ì—ì„œë§Œ ë³¼ë¥¨ ë‚®ì¶”ê¸°, ë‚˜ë¨¸ì§€ëŠ” ìœ ì§€
        ducking_volume = original_volume  # ë”ë¹™ ì¤‘ ë³¼ë¥¨ (ë‚®ìŒ)
        normal_volume = min(0.7, original_volume * 4)  # ë”ë¹™ ì—†ì„ ë•Œ ë³¼ë¥¨ (ë†’ìŒ)
        
        # ë³¼ë¥¨ ë³€í™” êµ¬ê°„ ìƒì„±
        volume_expr_parts = []
        
        for seg in segments:
            start = seg["start"]
            end = seg["end"]
            # ë”ë¹™ êµ¬ê°„: ë‚®ì€ ë³¼ë¥¨
            volume_expr_parts.append(f"between(t,{start},{end})*{ducking_volume}")
        
        # ë”ë¹™ ì—†ëŠ” êµ¬ê°„: ë†’ì€ ë³¼ë¥¨
        # volume = normal_volume * (1 - any_ducking) + ducking_volume * any_ducking
        ducking_expr = "+".join(volume_expr_parts) if volume_expr_parts else "0"
        volume_filter = f"volume='if(gt({ducking_expr},0),{ducking_volume},{normal_volume})':eval=frame"
    else:
        volume_filter = f"volume={original_volume}"
    
    # ë³µì¡í•œ í•„í„° ì²´ì¸ êµ¬ì„±
    inputs = ["-i", original_audio]
    filter_parts = []
    
    # ê° TTS íŒŒì¼ ì…ë ¥ ì¶”ê°€
    for i, seg in enumerate(segments):
        inputs.extend(["-i", seg["tts_path"]])
    
    # ì›ë³¸ ì˜¤ë””ì˜¤ ë³¼ë¥¨ ì¡°ì ˆ (ìë™ ë•í‚¹ ì ìš©)
    filter_parts.append(f"[0:a]{volume_filter}[orig]")
    
    # ê° ë”ë¹™ ì„¸ê·¸ë¨¼íŠ¸ì— ë”œë ˆì´ ì ìš©
    overlay_inputs = ["[orig]"]
    for i, seg in enumerate(segments):
        delay_ms = int(seg["start"] * 1000)
        filter_parts.append(
            f"[{i+1}:a]adelay={delay_ms}|{delay_ms}[dub{i}]"
        )
        overlay_inputs.append(f"[dub{i}]")
    
    # ëª¨ë“  íŠ¸ë™ ë¯¹ìŠ¤
    filter_parts.append(
        f"{''.join(overlay_inputs)}amix=inputs={len(segments)+1}:duration=longest:normalize=0[out]"
    )
    
    filter_complex = ";".join(filter_parts)
    
    cmd = ["ffmpeg", "-y"] + inputs + [
        "-filter_complex", filter_complex,
        "-map", "[out]",
        "-c:a", "libmp3lame", "-q:a", "2",
        output_path
    ]
    
    subprocess.run(cmd, capture_output=True, check=True)
    print("[ë¯¹ì‹±] ì™„ë£Œ")
    return output_path


def replace_video_audio(video_path: str, audio_path: str, output_path: str):
    """ì˜ìƒì˜ ì˜¤ë””ì˜¤ êµì²´"""
    cmd = [
        "ffmpeg", "-y",
        "-i", video_path,
        "-i", audio_path,
        "-c:v", "copy",
        "-map", "0:v:0", "-map", "1:a:0",
        "-shortest",
        output_path
    ]
    subprocess.run(cmd, capture_output=True, check=True)
    return output_path


def format_srt_time(seconds: float) -> str:
    """ì´ˆë¥¼ SRT ì‹œê°„ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (HH:MM:SS,mmm)"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millis = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"


def generate_srt(segments: list, output_path: str, dual: bool = False) -> str:
    """SRT ìë§‰ íŒŒì¼ ìƒì„± (ì´ì¤‘ ìë§‰ ì§€ì›)"""
    
    with open(output_path, "w", encoding="utf-8") as f:
        for i, seg in enumerate(segments, 1):
            start_time = format_srt_time(seg["start"])
            end_time = format_srt_time(seg["end"])
            
            if dual:
                # ì´ì¤‘ ìë§‰: ì˜ì–´ ì›ë³¸ + í•œêµ­ì–´ ë²ˆì—­
                original = seg.get("text", "")
                translated = seg.get("translated", "")
                text = f"{original}\n{translated}"
            else:
                # ë‹¨ì¼ ìë§‰: ë²ˆì—­ë§Œ
                text = seg.get("translated", seg.get("text", ""))
            
            f.write(f"{i}\n")
            f.write(f"{start_time} --> {end_time}\n")
            f.write(f"{text}\n\n")
    
    return output_path


def burn_subtitles(video_path: str, srt_path: str, output_path: str,
                   fontsize: int = 24, fontcolor: str = "white",
                   outline: int = 2, font: str = "NanumGothic") -> str:
    """ìë§‰ì„ ì˜ìƒì— í•˜ë“œì½”ë”© (burn-in)"""
    print("[ìë§‰] ìë§‰ í•©ì„± ì¤‘...")
    
    # Windows ê²½ë¡œ ì´ìŠ¤ì¼€ì´í”„
    srt_escaped = srt_path.replace("\\", "/").replace(":", "\\:")
    
    # ìë§‰ ìŠ¤íƒ€ì¼
    style = f"FontSize={fontsize},FontName={font},PrimaryColour=&Hffffff,OutlineColour=&H000000,Outline={outline},Shadow=1"
    
    cmd = [
        "ffmpeg", "-y",
        "-i", video_path,
        "-vf", f"subtitles='{srt_escaped}':force_style='{style}'",
        "-c:a", "copy",
        output_path
    ]
    
    result = subprocess.run(cmd, capture_output=True, text=True)
    if result.returncode != 0:
        print(f"[!] ìë§‰ í•©ì„± ì‹¤íŒ¨, ê¸°ë³¸ ìŠ¤íƒ€ì¼ë¡œ ì¬ì‹œë„...")
        # í°íŠ¸ ì—†ì„ ê²½ìš° ê¸°ë³¸ ìŠ¤íƒ€ì¼ë¡œ ì¬ì‹œë„
        cmd = [
            "ffmpeg", "-y",
            "-i", video_path,
            "-vf", f"subtitles='{srt_escaped}'",
            "-c:a", "copy",
            output_path
        ]
        subprocess.run(cmd, capture_output=True, check=True)
    
    print("[ìë§‰] ì™„ë£Œ")
    return output_path


def process_single_video(input_path: Path, output_path: str, args) -> bool:
    """ë‹¨ì¼ ì˜ìƒ ì²˜ë¦¬ (ë°°ì¹˜ìš©)"""
    start_time = time.time()
    
    try:
        print(f"\n{'='*50}")
        print(f"ì²˜ë¦¬ ì¤‘: {input_path.name}")
        print(f"{'='*50}\n")
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬
        if args.keep_temp:
            temp_dir = f"./temp_dub_{input_path.stem}"
            os.makedirs(temp_dir, exist_ok=True)
        else:
            temp_dir_obj = tempfile.TemporaryDirectory()
            temp_dir = temp_dir_obj.name
        
        try:
            # 1. ì˜¤ë””ì˜¤ ì¶”ì¶œ
            print("[1/7] ì˜¤ë””ì˜¤ ì¶”ì¶œ ì¤‘...")
            audio_wav = os.path.join(temp_dir, "original.wav")
            extract_audio_wav(str(input_path), audio_wav)
            
            # 2. VAD
            print("[2/7] VAD ìŒì„± êµ¬ê°„ ê°ì§€ ì¤‘...")
            speech_segments = detect_speech_segments(audio_wav)
            
            if not speech_segments:
                print("[!] ìŒì„±ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
                return False
            
            # 3. Whisper
            print("[3/7] ìŒì„± êµ¬ê°„ í…ìŠ¤íŠ¸ ë³€í™˜ ì¤‘...")
            audio_mp3 = os.path.join(temp_dir, "original.mp3")
            subprocess.run([
                "ffmpeg", "-y", "-i", audio_wav,
                "-acodec", "libmp3lame", "-q:a", "2", audio_mp3
            ], capture_output=True, check=True)
            
            segments = transcribe_speech_segments(audio_mp3, speech_segments, temp_dir)
            
            if not segments:
                print("[!] í…ìŠ¤íŠ¸ ë³€í™˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤!")
                return False
            
            # 4. ë²ˆì—­
            print("[4/7] ë²ˆì—­ ì¤‘...")
            segments = translate_segments(segments, args.lang, args.tone)
            
            # 5. TTS (ìŒì„± í´ë¡œë‹ ë˜ëŠ” ì„ íƒí•œ ëª©ì†Œë¦¬)
            print("[5/7] TTS ìƒì„± ì¤‘...")
            
            voice_id = None
            if hasattr(args, 'clone_voice') and args.clone_voice:
                # ìŒì„± í´ë¡œë‹
                print("[í´ë¡œë‹] ì›ë³¸ í™”ì ìŒì„± í´ë¡œë‹ ì‹œì‘...")
                sample_path = os.path.join(temp_dir, "voice_sample.mp3")
                extract_voice_sample(audio_mp3, speech_segments, sample_path)
                voice_id = clone_voice_elevenlabs([sample_path], f"cloned_{input_path.stem}")
                if not voice_id:
                    print("[í´ë¡œë‹] ì‹¤íŒ¨, ê¸°ë³¸ ëª©ì†Œë¦¬ ì‚¬ìš©")
            elif hasattr(args, 'voice') and args.voice:
                # ì„ íƒí•œ ëª©ì†Œë¦¬
                voice_id = ELEVENLABS_VOICES.get(args.voice)
                print(f"[TTS] ì„ íƒí•œ ëª©ì†Œë¦¬: {args.voice}")
            
            segments = process_tts_with_sync(segments, temp_dir, voice_id)
            
            # 6. ìë§‰
            srt_path = os.path.join(temp_dir, "subtitle_ko.srt")
            generate_srt(segments, srt_path, dual=args.dual_sub)
            
            output_srt = f"{Path(output_path).stem}.srt"
            import shutil
            shutil.copy(srt_path, output_srt)
            
            # 7. í•©ì„±
            if args.subtitle_only:
                print("[7/7] ìë§‰ë§Œ í•©ì„± ì¤‘...")
                burn_subtitles(str(input_path), srt_path, output_path, fontsize=args.fontsize)
            else:
                print("[7/7] ìµœì¢… í•©ì„± ì¤‘...")
                mixed_audio = os.path.join(temp_dir, "mixed.mp3")
                auto_ducking = not args.no_ducking
                mix_dubbed_audio(audio_mp3, segments, mixed_audio, args.original_volume, auto_ducking)
                
                dubbed_video = os.path.join(temp_dir, "dubbed_no_sub.mp4")
                replace_video_audio(str(input_path), mixed_audio, dubbed_video)
                
                if args.subtitle or args.dual_sub:
                    burn_subtitles(dubbed_video, srt_path, output_path, fontsize=args.fontsize)
                else:
                    shutil.copy(dubbed_video, output_path)
            
            elapsed = time.time() - start_time
            elapsed_min = int(elapsed // 60)
            elapsed_sec = int(elapsed % 60)
            
            print(f"[OK] ì™„ë£Œ: {output_path}")
            print(f"[OK] ì†Œìš”ì‹œê°„: {elapsed_min}ë¶„ {elapsed_sec}ì´ˆ")
            
            # í…”ë ˆê·¸ë¨ ì•Œë¦¼
            if hasattr(args, 'notify') and args.notify:
                notify_msg = f"ğŸ¬ <b>ë”ë¹™ ì™„ë£Œ!</b>\n\n"
                notify_msg += f"ğŸ“ íŒŒì¼: {input_path.name}\n"
                notify_msg += f"â± ì†Œìš”ì‹œê°„: {elapsed_min}ë¶„ {elapsed_sec}ì´ˆ\n"
                notify_msg += f"âœ… ì¶œë ¥: {Path(output_path).name}"
                
                send_telegram_notification(
                    notify_msg,
                    args.telegram_chat_id,
                    args.telegram_bot_token
                )
                print("[ì•Œë¦¼] í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ")
            
            return True
            
        finally:
            if not args.keep_temp and 'temp_dir_obj' in dir():
                temp_dir_obj.cleanup()
                
    except Exception as e:
        print(f"[ERROR] {input_path.name}: {e}")
        return False


def main():
    parser = argparse.ArgumentParser(description="Smart Dubbing Pipeline v3 (VAD)")
    parser.add_argument("input", nargs="?", help="ì…ë ¥ ì˜ìƒ íŒŒì¼ ë˜ëŠ” ìœ íŠœë¸Œ URL")
    parser.add_argument("-o", "--output", help="ì¶œë ¥ íŒŒì¼ëª…")
    parser.add_argument("--batch", help="ë°°ì¹˜ ì²˜ë¦¬: í´ë” ê²½ë¡œ ì§€ì •")
    parser.add_argument("--batch-output", help="ë°°ì¹˜ ì¶œë ¥ í´ë” (ê¸°ë³¸: input_dubbed)")
    parser.add_argument("-l", "--lang", default="Korean", help="íƒ€ê²Ÿ ì–¸ì–´")
    parser.add_argument("--original-volume", type=float, default=0.15, help="ì›ë³¸ ì˜¤ë””ì˜¤ ë³¼ë¥¨")
    parser.add_argument("--subtitle", action="store_true", help="ìë§‰ í¬í•¨")
    parser.add_argument("--subtitle-only", action="store_true", help="ìë§‰ë§Œ ìƒì„± (ë”ë¹™ ì—†ì´)")
    parser.add_argument("--dual-sub", action="store_true", help="ì´ì¤‘ ìë§‰ (ì˜ì–´+í•œêµ­ì–´)")
    parser.add_argument("--fontsize", type=int, default=24, help="ìë§‰ í°íŠ¸ í¬ê¸°")
    parser.add_argument("--tone", choices=["formal", "casual", "narration"], default="formal",
                        help="ë§íˆ¬ ì„ íƒ: formal(ì¡´ëŒ“ë§), casual(ë°˜ë§), narration(ë‚˜ë ˆì´ì…˜ì²´)")
    parser.add_argument("--auto-ducking", action="store_true", default=True,
                        help="ìë™ ë³¼ë¥¨ ì¡°ì ˆ (ë”ë¹™ êµ¬ê°„ì—ì„œ ë°°ê²½ìŒ ë‚®ì¶¤)")
    parser.add_argument("--no-ducking", action="store_true",
                        help="ìë™ ë³¼ë¥¨ ì¡°ì ˆ ë¹„í™œì„±í™”")
    parser.add_argument("--notify", action="store_true", help="ì™„ë£Œ ì‹œ í…”ë ˆê·¸ë¨ ì•Œë¦¼")
    parser.add_argument("--clone-voice", action="store_true", help="ì›ë³¸ í™”ì ìŒì„± í´ë¡œë‹")
    parser.add_argument("--voice", choices=["male_1", "male_2", "female_1", "female_2"],
                        help="TTS ëª©ì†Œë¦¬ ì„ íƒ (í´ë¡œë‹ ì•ˆ í•  ë•Œ)")
    parser.add_argument("--telegram-chat-id", default="6329826367", help="í…”ë ˆê·¸ë¨ Chat ID")
    parser.add_argument("--telegram-bot-token", 
                        default="8293841489:AAE6XG6x5v0Prgs0bqsVMlK9Fe_K46ESWng",
                        help="í…”ë ˆê·¸ë¨ Bot Token")
    parser.add_argument("--keep-temp", action="store_true", help="ì„ì‹œ íŒŒì¼ ìœ ì§€")
    args = parser.parse_args()
    
    # ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ
    if args.batch:
        batch_dir = Path(args.batch)
        if not batch_dir.exists():
            print(f"[!] í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {batch_dir}")
            sys.exit(1)
        
        # ì˜ìƒ íŒŒì¼ ì°¾ê¸°
        video_extensions = ['.mp4', '.mkv', '.avi', '.mov', '.webm']
        video_files = [f for f in batch_dir.iterdir() 
                      if f.suffix.lower() in video_extensions]
        
        if not video_files:
            print(f"[!] ì˜ìƒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {batch_dir}")
            sys.exit(1)
        
        # ì¶œë ¥ í´ë”
        output_dir = Path(args.batch_output) if args.batch_output else batch_dir / "dubbed"
        output_dir.mkdir(exist_ok=True)
        
        print(f"\n{'='*50}")
        print(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘")
        print(f"{'='*50}")
        print(f"ì…ë ¥ í´ë”: {batch_dir}")
        print(f"ì¶œë ¥ í´ë”: {output_dir}")
        print(f"ì˜ìƒ ê°œìˆ˜: {len(video_files)}ê°œ")
        print(f"{'='*50}\n")
        
        success = 0
        failed = 0
        
        for i, video_file in enumerate(video_files, 1):
            print(f"\n[{i}/{len(video_files)}] {video_file.name}")
            output_path = output_dir / f"{video_file.stem}_dubbed_ko{video_file.suffix}"
            
            if process_single_video(video_file, str(output_path), args):
                success += 1
            else:
                failed += 1
        
        print(f"\n{'='*50}")
        print(f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"ì„±ê³µ: {success}ê°œ / ì‹¤íŒ¨: {failed}ê°œ")
        print(f"{'='*50}\n")
        
        # ë°°ì¹˜ ì™„ë£Œ ì•Œë¦¼
        if args.notify:
            notify_msg = f"ğŸ“¦ <b>ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!</b>\n\n"
            notify_msg += f"ğŸ“ í´ë”: {batch_dir.name}\n"
            notify_msg += f"âœ… ì„±ê³µ: {success}ê°œ\n"
            notify_msg += f"âŒ ì‹¤íŒ¨: {failed}ê°œ"
            
            send_telegram_notification(
                notify_msg,
                args.telegram_chat_id,
                args.telegram_bot_token
            )
            print("[ì•Œë¦¼] í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ")
        
        return
    
    # ë‹¨ì¼ íŒŒì¼ ëª¨ë“œ - input í•„ìˆ˜
    if not args.input:
        print("[!] ì…ë ¥ íŒŒì¼ì„ ì§€ì •í•´ì£¼ì„¸ìš”.")
        print("ì‚¬ìš©ë²•: python smart_dub_v3.py <ì˜ìƒíŒŒì¼ ë˜ëŠ” ìœ íŠœë¸ŒURL>")
        print("ë°°ì¹˜:   python smart_dub_v3.py --batch <í´ë”ê²½ë¡œ>")
        sys.exit(1)
    
    # ìœ íŠœë¸Œ URL ì²˜ë¦¬
    if is_youtube_url(args.input):
        print(f"\n[YouTube] URL ê°ì§€ë¨!")
        temp_download_dir = "./temp_youtube"
        os.makedirs(temp_download_dir, exist_ok=True)
        downloaded_file = download_youtube(args.input, temp_download_dir)
        input_path = Path(downloaded_file)
    else:
        input_path = Path(args.input)
        if not input_path.exists():
            print(f"[!] íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}")
            sys.exit(1)
    
    output_path = args.output or f"{input_path.stem}_dubbed_ko{input_path.suffix}"
    
    # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
    success = process_single_video(input_path, output_path, args)
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
