{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ¬ Auto-Dubbing Pipeline (Colab GPU)\n",
        "\n",
        "ì˜ì–´ ì˜ìƒ â†’ í•œêµ­ì–´ ë”ë¹™ + ìë§‰\n",
        "\n",
        "**ì‚¬ìš©ë²•:**\n",
        "1. ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ â†’ **GPU (T4)** ì„ íƒ\n",
        "2. ì…€ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰\n",
        "3. ìœ íŠœë¸Œ URL ì…ë ¥ ë˜ëŠ” íŒŒì¼ ì—…ë¡œë“œ"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1ï¸âƒ£ ì„¤ì¹˜ (ì²˜ìŒ í•œ ë²ˆë§Œ)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
        "!pip install -q openai requests yt-dlp torch torchaudio\n",
        "\n",
        "# numpy ë‹¤ìš´ê·¸ë ˆì´ë“œ (pyannote í˜¸í™˜ì„±)\n",
        "!pip install -q 'numpy<2.0'\n",
        "\n",
        "# pyannote ì„¤ì¹˜\n",
        "!pip install -q pyannote.audio\n",
        "\n",
        "!apt-get install -y ffmpeg > /dev/null 2>&1\n",
        "\n",
        "print(\"\")\n",
        "print(\"=\"*50)\n",
        "print(\"âœ… ì„¤ì¹˜ ì™„ë£Œ!\")\n",
        "print(\"=\"*50)\n",
        "print(\"âš ï¸ ì¤‘ìš”: ì§€ê¸ˆ ë°”ë¡œ ëŸ°íƒ€ì„ ë‹¤ì‹œ ì‹œì‘í•˜ì„¸ìš”!\")\n",
        "print(\"   ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ë‹¤ì‹œ ì‹œì‘\")\n",
        "print(\"   ê·¸ ë‹¤ìŒ 2ë²ˆ ì…€(API í‚¤ ì„¤ì •)ë¶€í„° ì‹¤í–‰\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2ï¸âƒ£ API í‚¤ ì„¤ì •"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# API í‚¤ ì„¤ì • (ë³¸ì¸ í‚¤ë¡œ êµì²´í•˜ì„¸ìš”)\n",
        "os.environ['OPENAI_API_KEY'] = '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'\n",
        "os.environ['ELEVENLABS_API_KEY'] = 'sk_7b0a163f718c23222429625faebe9dabf428825ebc36d6c2'\n",
        "os.environ['HF_TOKEN'] = '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'\n",
        "\n",
        "print(\"âœ… API í‚¤ ì„¤ì • ì™„ë£Œ!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3ï¸âƒ£ GPU í™•ì¸"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {gpu_name}\")\n",
        "else:\n",
        "    print(\"âš ï¸ GPU ì—†ìŒ! ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ â†’ GPU ì„ íƒí•˜ì„¸ìš”\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4ï¸âƒ£ ë”ë¹™ íŒŒì´í”„ë¼ì¸ ì½”ë“œ"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import subprocess\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torchaudio\n",
        "from openai import OpenAI\n",
        "import requests\n",
        "\n",
        "# ì„¤ì •\n",
        "ELEVENLABS_API_KEY = os.environ.get('ELEVENLABS_API_KEY')\n",
        "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
        "HF_TOKEN = os.environ.get('HF_TOKEN')\n",
        "\n",
        "# ElevenLabs ëª©ì†Œë¦¬\n",
        "VOICES = {\n",
        "    \"male_1\": \"pNInz6obpgDQGcFmaJgB\",    # Adam\n",
        "    \"male_2\": \"VR6AewLTigWG4xSOukaG\",    # Arnold\n",
        "    \"female_1\": \"pFZP5JQG7iQjIQuC4Bku\",  # Lily\n",
        "    \"female_2\": \"21m00Tcm4TlvDq8ikWAM\",  # Rachel\n",
        "}\n",
        "\n",
        "SPEAKER_VOICE_MAP = {\n",
        "    \"SPEAKER_00\": \"male_1\",\n",
        "    \"SPEAKER_01\": \"female_1\",\n",
        "    \"SPEAKER_02\": \"male_2\",\n",
        "    \"SPEAKER_03\": \"female_2\",\n",
        "}\n",
        "\n",
        "def download_youtube(url, output_dir):\n",
        "    \"\"\"ìœ íŠœë¸Œ ë‹¤ìš´ë¡œë“œ\"\"\"\n",
        "    print(f\"[YouTube] ë‹¤ìš´ë¡œë“œ ì¤‘: {url}\")\n",
        "    cmd = [\n",
        "        \"yt-dlp\", \"-f\", \"bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]\",\n",
        "        \"--merge-output-format\", \"mp4\",\n",
        "        \"-o\", f\"{output_dir}/%(title)s.%(ext)s\",\n",
        "        url\n",
        "    ]\n",
        "    subprocess.run(cmd, check=True)\n",
        "    files = list(Path(output_dir).glob(\"*.mp4\"))\n",
        "    if files:\n",
        "        print(f\"[YouTube] ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {files[0].name}\")\n",
        "        return str(files[0])\n",
        "    raise Exception(\"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨\")\n",
        "\n",
        "def extract_audio(video_path, audio_path):\n",
        "    \"\"\"ì˜¤ë””ì˜¤ ì¶”ì¶œ (WAV)\"\"\"\n",
        "    cmd = [\"ffmpeg\", \"-y\", \"-i\", video_path, \"-vn\", \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \"-ac\", \"1\", audio_path]\n",
        "    subprocess.run(cmd, capture_output=True, check=True)\n",
        "    return audio_path\n",
        "\n",
        "def detect_speech_vad(audio_path):\n",
        "    \"\"\"Silero VADë¡œ ìŒì„± êµ¬ê°„ ê°ì§€\"\"\"\n",
        "    print(\"[VAD] ìŒì„± êµ¬ê°„ ê°ì§€ ì¤‘...\")\n",
        "    model, utils = torch.hub.load('snakers4/silero-vad', 'silero_vad', trust_repo=True)\n",
        "    (get_speech_timestamps, _, read_audio, _, _) = utils\n",
        "    \n",
        "    wav = read_audio(audio_path, sampling_rate=16000)\n",
        "    timestamps = get_speech_timestamps(wav, model, sampling_rate=16000)\n",
        "    \n",
        "    segments = []\n",
        "    for ts in timestamps:\n",
        "        start = ts['start'] / 16000\n",
        "        end = ts['end'] / 16000\n",
        "        segments.append({'start': round(start, 2), 'end': round(end, 2), 'duration': round(end - start, 2)})\n",
        "    \n",
        "    print(f\"[VAD] {len(segments)}ê°œ ìŒì„± êµ¬ê°„ ê°ì§€\")\n",
        "    return segments\n",
        "\n",
        "def speaker_diarization(audio_path):\n",
        "    \"\"\"pyannote í™”ì ë¶„ë¦¬ (GPU ê°€ì†)\"\"\"\n",
        "    print(\"[í™”ìë¶„ë¦¬] ë¶„ì„ ì¤‘... (GPU ì‚¬ìš©)\")\n",
        "    from pyannote.audio import Pipeline\n",
        "    \n",
        "    pipeline = Pipeline.from_pretrained(\n",
        "        \"pyannote/speaker-diarization-3.1\",\n",
        "        token=HF_TOKEN\n",
        "    )\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        pipeline = pipeline.to(torch.device(\"cuda\"))\n",
        "    \n",
        "    diarization = pipeline(audio_path)\n",
        "    \n",
        "    speaker_segments = {}\n",
        "    speakers = set()\n",
        "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
        "        speaker_segments[(round(turn.start, 2), round(turn.end, 2))] = speaker\n",
        "        speakers.add(speaker)\n",
        "    \n",
        "    print(f\"[í™”ìë¶„ë¦¬] ì™„ë£Œ! {len(speakers)}ëª… ê°ì§€: {', '.join(sorted(speakers))}\")\n",
        "    return speaker_segments\n",
        "\n",
        "def assign_speakers(segments, diarization):\n",
        "    \"\"\"ì„¸ê·¸ë¨¼íŠ¸ì— í™”ì í• ë‹¹\"\"\"\n",
        "    for seg in segments:\n",
        "        seg_mid = (seg['start'] + seg['end']) / 2\n",
        "        best_speaker = 'SPEAKER_00'\n",
        "        max_overlap = 0\n",
        "        \n",
        "        for (start, end), speaker in diarization.items():\n",
        "            overlap = max(0, min(seg['end'], end) - max(seg['start'], start))\n",
        "            if overlap > max_overlap:\n",
        "                max_overlap = overlap\n",
        "                best_speaker = speaker\n",
        "        \n",
        "        seg['speaker'] = best_speaker\n",
        "    return segments\n",
        "\n",
        "def transcribe_segments(audio_path, segments, temp_dir):\n",
        "    \"\"\"Whisperë¡œ í…ìŠ¤íŠ¸ ë³€í™˜\"\"\"\n",
        "    print(\"[Whisper] í…ìŠ¤íŠ¸ ë³€í™˜ ì¤‘...\")\n",
        "    client = OpenAI()\n",
        "    \n",
        "    for i, seg in enumerate(segments):\n",
        "        print(f\"  {i+1}/{len(segments)}\", end=\"\\r\")\n",
        "        seg_path = f\"{temp_dir}/seg_{i:04d}.mp3\"\n",
        "        cmd = [\"ffmpeg\", \"-y\", \"-i\", audio_path, \"-ss\", str(seg['start']), \"-to\", str(seg['end']), \"-q:a\", \"2\", seg_path]\n",
        "        subprocess.run(cmd, capture_output=True)\n",
        "        \n",
        "        with open(seg_path, \"rb\") as f:\n",
        "            text = client.audio.transcriptions.create(model=\"whisper-1\", file=f, response_format=\"text\")\n",
        "        seg['text'] = text.strip()\n",
        "    \n",
        "    print(f\"\\n[Whisper] {len(segments)}ê°œ ì™„ë£Œ\")\n",
        "    return [s for s in segments if s.get('text')]\n",
        "\n",
        "def translate_segments(segments, tone=\"formal\"):\n",
        "    \"\"\"GPTë¡œ ë²ˆì—­\"\"\"\n",
        "    print(f\"[ë²ˆì—­] ë²ˆì—­ ì¤‘... (ë§íˆ¬: {tone})\")\n",
        "    client = OpenAI()\n",
        "    \n",
        "    tone_desc = {\n",
        "        \"formal\": \"ì¡´ëŒ“ë§ (~ìŠµë‹ˆë‹¤/~ìš”)\",\n",
        "        \"casual\": \"ë°˜ë§ (~í•´/~ì•¼)\",\n",
        "        \"narration\": \"ë‚˜ë ˆì´ì…˜ì²´ (~ë‹¤/~í–ˆë‹¤)\"\n",
        "    }.get(tone, \"ì¡´ëŒ“ë§\")\n",
        "    \n",
        "    texts = [{\"id\": i, \"text\": s[\"text\"], \"duration\": s[\"duration\"]} for i, s in enumerate(segments)]\n",
        "    \n",
        "    prompt = f\"\"\"Translate to Korean ({tone_desc}). Keep translations speakable within the duration.\n",
        "Return JSON array with \"id\" and \"translation\".\n",
        "\n",
        "{json.dumps(texts, ensure_ascii=False)}\"\"\"\n",
        "    \n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    \n",
        "    content = response.choices[0].message.content\n",
        "    if \"```json\" in content:\n",
        "        content = content.split(\"```json\")[1].split(\"```\")[0]\n",
        "    elif \"```\" in content:\n",
        "        content = content.split(\"```\")[1].split(\"```\")[0]\n",
        "    \n",
        "    translations = {item[\"id\"]: item[\"translation\"] for item in json.loads(content.strip())}\n",
        "    for i, seg in enumerate(segments):\n",
        "        seg[\"translated\"] = translations.get(i, seg[\"text\"])\n",
        "    \n",
        "    print(\"[ë²ˆì—­] ì™„ë£Œ\")\n",
        "    return segments\n",
        "\n",
        "def generate_tts(text, output_path, voice_id):\n",
        "    \"\"\"ElevenLabs TTS\"\"\"\n",
        "    url = f\"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}\"\n",
        "    response = requests.post(url, headers={\"xi-api-key\": ELEVENLABS_API_KEY, \"Content-Type\": \"application/json\"},\n",
        "                            json={\"text\": text, \"model_id\": \"eleven_multilingual_v2\"})\n",
        "    response.raise_for_status()\n",
        "    with open(output_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    return output_path\n",
        "\n",
        "def get_duration(path):\n",
        "    \"\"\"ì˜¤ë””ì˜¤ ê¸¸ì´\"\"\"\n",
        "    result = subprocess.run([\"ffprobe\", \"-v\", \"error\", \"-show_entries\", \"format=duration\", \"-of\", \"default=noprint_wrappers=1:nokey=1\", path], capture_output=True, text=True)\n",
        "    return float(result.stdout.strip())\n",
        "\n",
        "def process_tts(segments, temp_dir, use_diarization=True):\n",
        "    \"\"\"TTS ìƒì„±\"\"\"\n",
        "    print(\"[TTS] ìŒì„± ìƒì„± ì¤‘...\")\n",
        "    \n",
        "    for i, seg in enumerate(segments):\n",
        "        print(f\"  {i+1}/{len(segments)}\", end=\"\\r\")\n",
        "        \n",
        "        if use_diarization:\n",
        "            speaker = seg.get('speaker', 'SPEAKER_00')\n",
        "            voice_key = SPEAKER_VOICE_MAP.get(speaker, 'male_1')\n",
        "        else:\n",
        "            voice_key = 'female_1'\n",
        "        \n",
        "        voice_id = VOICES[voice_key]\n",
        "        tts_path = f\"{temp_dir}/tts_{i:04d}.mp3\"\n",
        "        generate_tts(seg[\"translated\"], tts_path, voice_id)\n",
        "        seg[\"tts_path\"] = tts_path\n",
        "    \n",
        "    print(f\"\\n[TTS] {len(segments)}ê°œ ì™„ë£Œ\")\n",
        "    return segments\n",
        "\n",
        "def mix_audio(original_audio, segments, output_path, original_volume=0.15):\n",
        "    \"\"\"ì˜¤ë””ì˜¤ ë¯¹ì‹±\"\"\"\n",
        "    print(\"[ë¯¹ì‹±] ì˜¤ë””ì˜¤ ë¯¹ì‹± ì¤‘...\")\n",
        "    \n",
        "    inputs = [\"-i\", original_audio]\n",
        "    filter_parts = [f\"[0:a]volume={original_volume}[orig]\"]\n",
        "    overlay_inputs = [\"[orig]\"]\n",
        "    \n",
        "    for i, seg in enumerate(segments):\n",
        "        inputs.extend([\"-i\", seg[\"tts_path\"]])\n",
        "        delay_ms = int(seg[\"start\"] * 1000)\n",
        "        filter_parts.append(f\"[{i+1}:a]adelay={delay_ms}|{delay_ms}[dub{i}]\")\n",
        "        overlay_inputs.append(f\"[dub{i}]\")\n",
        "    \n",
        "    filter_parts.append(f\"{''.join(overlay_inputs)}amix=inputs={len(segments)+1}:duration=longest:normalize=0[out]\")\n",
        "    \n",
        "    cmd = [\"ffmpeg\", \"-y\"] + inputs + [\"-filter_complex\", \";\".join(filter_parts), \"-map\", \"[out]\", \"-c:a\", \"libmp3lame\", output_path]\n",
        "    subprocess.run(cmd, capture_output=True, check=True)\n",
        "    print(\"[ë¯¹ì‹±] ì™„ë£Œ\")\n",
        "    return output_path\n",
        "\n",
        "def replace_audio(video_path, audio_path, output_path):\n",
        "    \"\"\"ì˜ìƒ ì˜¤ë””ì˜¤ êµì²´\"\"\"\n",
        "    cmd = [\"ffmpeg\", \"-y\", \"-i\", video_path, \"-i\", audio_path, \"-c:v\", \"copy\", \"-map\", \"0:v:0\", \"-map\", \"1:a:0\", \"-shortest\", output_path]\n",
        "    subprocess.run(cmd, capture_output=True, check=True)\n",
        "    return output_path\n",
        "\n",
        "def generate_srt(segments, output_path):\n",
        "    \"\"\"SRT ìë§‰ ìƒì„±\"\"\"\n",
        "    def fmt_time(s):\n",
        "        h, m, sec = int(s//3600), int((s%3600)//60), int(s%60)\n",
        "        ms = int((s%1)*1000)\n",
        "        return f\"{h:02d}:{m:02d}:{sec:02d},{ms:03d}\"\n",
        "    \n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for i, seg in enumerate(segments, 1):\n",
        "            f.write(f\"{i}\\n{fmt_time(seg['start'])} --> {fmt_time(seg['end'])}\\n{seg['translated']}\\n\\n\")\n",
        "    return output_path\n",
        "\n",
        "print(\"âœ… í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5ï¸âƒ£ ë”ë¹™ ì‹¤í–‰"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ğŸ¬ ë”ë¹™ ì„¤ì •\n",
        "youtube_url = \"https://www.youtube.com/watch?v=-dtGzB4zqlc\" #@param {type:\"string\"}\n",
        "use_diarization = True #@param {type:\"boolean\"}\n",
        "tone = \"formal\" #@param [\"formal\", \"casual\", \"narration\"]\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "# ì„ì‹œ í´ë”\n",
        "temp_dir = tempfile.mkdtemp()\n",
        "print(f\"ì„ì‹œ í´ë”: {temp_dir}\")\n",
        "\n",
        "# 1. ìœ íŠœë¸Œ ë‹¤ìš´ë¡œë“œ\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"[1/7] ìœ íŠœë¸Œ ë‹¤ìš´ë¡œë“œ\")\n",
        "video_path = download_youtube(youtube_url, temp_dir)\n",
        "\n",
        "# 2. ì˜¤ë””ì˜¤ ì¶”ì¶œ\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"[2/7] ì˜¤ë””ì˜¤ ì¶”ì¶œ\")\n",
        "audio_wav = f\"{temp_dir}/audio.wav\"\n",
        "audio_mp3 = f\"{temp_dir}/audio.mp3\"\n",
        "extract_audio(video_path, audio_wav)\n",
        "subprocess.run([\"ffmpeg\", \"-y\", \"-i\", audio_wav, \"-q:a\", \"2\", audio_mp3], capture_output=True)\n",
        "\n",
        "# 3. VAD\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"[3/7] VAD ìŒì„± ê°ì§€\")\n",
        "segments = detect_speech_vad(audio_wav)\n",
        "\n",
        "# 4. í™”ì ë¶„ë¦¬ (ì˜µì…˜)\n",
        "if use_diarization:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"[4/7] í™”ì ë¶„ë¦¬ (GPU)\")\n",
        "    diarization = speaker_diarization(audio_wav)\n",
        "    segments = assign_speakers(segments, diarization)\n",
        "else:\n",
        "    print(\"\\n[4/7] í™”ì ë¶„ë¦¬ ê±´ë„ˆëœ€\")\n",
        "\n",
        "# 5. Whisper\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"[5/7] Whisper í…ìŠ¤íŠ¸ ë³€í™˜\")\n",
        "segments = transcribe_segments(audio_mp3, segments, temp_dir)\n",
        "\n",
        "# 6. ë²ˆì—­\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"[6/7] ë²ˆì—­\")\n",
        "segments = translate_segments(segments, tone)\n",
        "\n",
        "# 7. TTS\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"[7/7] TTS ìƒì„±\")\n",
        "segments = process_tts(segments, temp_dir, use_diarization)\n",
        "\n",
        "# 8. ë¯¹ì‹± & ìµœì¢… ì¶œë ¥\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"[8/8] ìµœì¢… í•©ì„±\")\n",
        "mixed_audio = f\"{temp_dir}/mixed.mp3\"\n",
        "mix_audio(audio_mp3, segments, mixed_audio)\n",
        "\n",
        "output_video = \"/content/dubbed_output.mp4\"\n",
        "output_srt = \"/content/dubbed_output.srt\"\n",
        "replace_audio(video_path, mixed_audio, output_video)\n",
        "generate_srt(segments, output_srt)\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"\\n\" + \"=\"*50)\n",
        "print(f\"âœ… ì™„ë£Œ! ì†Œìš”ì‹œê°„: {int(elapsed//60)}ë¶„ {int(elapsed%60)}ì´ˆ\")\n",
        "print(f\"ğŸ“¹ ì˜ìƒ: {output_video}\")\n",
        "print(f\"ğŸ“ ìë§‰: {output_srt}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6ï¸âƒ£ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# ì˜ìƒ ë‹¤ìš´ë¡œë“œ\n",
        "files.download('/content/dubbed_output.mp4')\n",
        "files.download('/content/dubbed_output.srt')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“º ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Video\n",
        "Video('/content/dubbed_output.mp4', width=640)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
