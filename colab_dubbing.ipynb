{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ¬ Auto-Dubbing Pipeline (Colab GPU)\n",
        "\n",
        "ì˜ì–´ ì˜ìƒ â†’ í•œêµ­ì–´ ë”ë¹™ + ìë§‰\n",
        "\n",
        "**ì‚¬ìš©ë²•:**\n",
        "1. ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ â†’ **GPU (T4)** ì„ íƒ\n",
        "2. ì…€ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰\n",
        "3. API í‚¤ ì…ë ¥\n",
        "4. ìœ íŠœë¸Œ URL ì…ë ¥"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1ï¸âƒ£ ì„¤ì¹˜"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
        "!pip install -q openai requests yt-dlp torch torchaudio\n",
        "!apt-get install -y ffmpeg > /dev/null 2>&1\n",
        "\n",
        "print(\"âœ… ì„¤ì¹˜ ì™„ë£Œ!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2ï¸âƒ£ API í‚¤ ì„¤ì • âš ï¸ í•„ìˆ˜!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#@title API í‚¤ ì…ë ¥ (ì‹¤í–‰ í›„ ì…ë ¥ì°½ì— í‚¤ ì…ë ¥)\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "print(\"API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”:\")\n",
        "print(\"(ì…ë ¥ ì‹œ í™”ë©´ì— í‘œì‹œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤)\")\n",
        "print()\n",
        "\n",
        "openai_key = getpass(\"OpenAI API Key: \")\n",
        "elevenlabs_key = getpass(\"ElevenLabs API Key: \")\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = openai_key\n",
        "os.environ['ELEVENLABS_API_KEY'] = elevenlabs_key\n",
        "\n",
        "print()\n",
        "print(\"âœ… API í‚¤ ì„¤ì • ì™„ë£Œ!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3ï¸âƒ£ GPU í™•ì¸"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {gpu_name}\")\n",
        "else:\n",
        "    print(\"âš ï¸ GPU ì—†ìŒ! ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ â†’ GPU ì„ íƒí•˜ì„¸ìš”\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4ï¸âƒ£ ë”ë¹™ íŒŒì´í”„ë¼ì¸ ì½”ë“œ"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import subprocess\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torchaudio\n",
        "from openai import OpenAI\n",
        "import requests\n",
        "\n",
        "# ì„¤ì •\n",
        "ELEVENLABS_API_KEY = os.environ.get('ELEVENLABS_API_KEY')\n",
        "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
        "\n",
        "# ElevenLabs ëª©ì†Œë¦¬\n",
        "VOICES = {\n",
        "    \"male_1\": \"pNInz6obpgDQGcFmaJgB\",    # Adam\n",
        "    \"male_2\": \"VR6AewLTigWG4xSOukaG\",    # Arnold\n",
        "    \"female_1\": \"pFZP5JQG7iQjIQuC4Bku\",  # Lily\n",
        "    \"female_2\": \"21m00Tcm4TlvDq8ikWAM\",  # Rachel\n",
        "}\n",
        "\n",
        "def download_youtube(url, output_dir):\n",
        "    \"\"\"ìœ íŠœë¸Œ ë‹¤ìš´ë¡œë“œ\"\"\"\n",
        "    print(f\"[YouTube] ë‹¤ìš´ë¡œë“œ ì¤‘: {url}\")\n",
        "    cmd = [\n",
        "        \"yt-dlp\", \"-f\", \"bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]\",\n",
        "        \"--merge-output-format\", \"mp4\",\n",
        "        \"-o\", f\"{output_dir}/%(title)s.%(ext)s\",\n",
        "        url\n",
        "    ]\n",
        "    subprocess.run(cmd, check=True)\n",
        "    files = list(Path(output_dir).glob(\"*.mp4\"))\n",
        "    if files:\n",
        "        print(f\"[YouTube] ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {files[0].name}\")\n",
        "        return str(files[0])\n",
        "    raise Exception(\"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨\")\n",
        "\n",
        "def extract_audio(video_path, audio_path):\n",
        "    \"\"\"ì˜¤ë””ì˜¤ ì¶”ì¶œ (WAV)\"\"\"\n",
        "    cmd = [\"ffmpeg\", \"-y\", \"-i\", video_path, \"-vn\", \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \"-ac\", \"1\", audio_path]\n",
        "    subprocess.run(cmd, capture_output=True, check=True)\n",
        "    return audio_path\n",
        "\n",
        "def detect_speech_vad(audio_path):\n",
        "    \"\"\"Silero VADë¡œ ìŒì„± êµ¬ê°„ ê°ì§€\"\"\"\n",
        "    print(\"[VAD] ìŒì„± êµ¬ê°„ ê°ì§€ ì¤‘...\")\n",
        "    model, utils = torch.hub.load('snakers4/silero-vad', 'silero_vad', trust_repo=True)\n",
        "    (get_speech_timestamps, _, read_audio, _, _) = utils\n",
        "    \n",
        "    wav = read_audio(audio_path, sampling_rate=16000)\n",
        "    timestamps = get_speech_timestamps(wav, model, sampling_rate=16000)\n",
        "    \n",
        "    segments = []\n",
        "    for ts in timestamps:\n",
        "        start = ts['start'] / 16000\n",
        "        end = ts['end'] / 16000\n",
        "        segments.append({'start': round(start, 2), 'end': round(end, 2), 'duration': round(end - start, 2)})\n",
        "    \n",
        "    print(f\"[VAD] {len(segments)}ê°œ ìŒì„± êµ¬ê°„ ê°ì§€\")\n",
        "    return segments\n",
        "\n",
        "def transcribe_segments(audio_path, segments, temp_dir):\n",
        "    \"\"\"Whisperë¡œ í…ìŠ¤íŠ¸ ë³€í™˜\"\"\"\n",
        "    print(\"[Whisper] í…ìŠ¤íŠ¸ ë³€í™˜ ì¤‘...\")\n",
        "    client = OpenAI()\n",
        "    \n",
        "    for i, seg in enumerate(segments):\n",
        "        print(f\"  {i+1}/{len(segments)}\", end=\"\\r\")\n",
        "        seg_path = f\"{temp_dir}/seg_{i:04d}.mp3\"\n",
        "        cmd = [\"ffmpeg\", \"-y\", \"-i\", audio_path, \"-ss\", str(seg['start']), \"-to\", str(seg['end']), \"-q:a\", \"2\", seg_path]\n",
        "        subprocess.run(cmd, capture_output=True)\n",
        "        \n",
        "        with open(seg_path, \"rb\") as f:\n",
        "            text = client.audio.transcriptions.create(model=\"whisper-1\", file=f, response_format=\"text\")\n",
        "        seg['text'] = text.strip()\n",
        "    \n",
        "    print(f\"\\n[Whisper] {len(segments)}ê°œ ì™„ë£Œ\")\n",
        "    return [s for s in segments if s.get('text')]\n",
        "\n",
        "def translate_segments(segments, tone=\"formal\"):\n",
        "    \"\"\"GPTë¡œ ë²ˆì—­\"\"\"\n",
        "    print(f\"[ë²ˆì—­] ë²ˆì—­ ì¤‘... (ë§íˆ¬: {tone})\")\n",
        "    client = OpenAI()\n",
        "    \n",
        "    tone_desc = {\n",
        "        \"formal\": \"ì¡´ëŒ“ë§ (~ìŠµë‹ˆë‹¤/~ìš”)\",\n",
        "        \"casual\": \"ë°˜ë§ (~í•´/~ì•¼)\",\n",
        "        \"narration\": \"ë‚˜ë ˆì´ì…˜ì²´ (~ë‹¤/~í–ˆë‹¤)\"\n",
        "    }.get(tone, \"ì¡´ëŒ“ë§\")\n",
        "    \n",
        "    texts = [{\"id\": i, \"text\": s[\"text\"], \"duration\": s[\"duration\"]} for i, s in enumerate(segments)]\n",
        "    \n",
        "    prompt = f\"\"\"Translate to Korean ({tone_desc}). Keep translations speakable within the duration.\n",
        "Return JSON array with \"id\" and \"translation\".\n",
        "\n",
        "{json.dumps(texts, ensure_ascii=False)}\"\"\"\n",
        "    \n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    \n",
        "    content = response.choices[0].message.content\n",
        "    if \"```json\" in content:\n",
        "        content = content.split(\"```json\")[1].split(\"```\")[0]\n",
        "    elif \"```\" in content:\n",
        "        content = content.split(\"```\")[1].split(\"```\")[0]\n",
        "    \n",
        "    translations = {item[\"id\"]: item[\"translation\"] for item in json.loads(content.strip())}\n",
        "    for i, seg in enumerate(segments):\n",
        "        seg[\"translated\"] = translations.get(i, seg[\"text\"])\n",
        "    \n",
        "    print(\"[ë²ˆì—­] ì™„ë£Œ\")\n",
        "    return segments\n",
        "\n",
        "def generate_tts(text, output_path, voice_id):\n",
        "    \"\"\"ElevenLabs TTS\"\"\"\n",
        "    url = f\"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}\"\n",
        "    response = requests.post(url, headers={\"xi-api-key\": ELEVENLABS_API_KEY, \"Content-Type\": \"application/json\"},\n",
        "                            json={\"text\": text, \"model_id\": \"eleven_multilingual_v2\"})\n",
        "    response.raise_for_status()\n",
        "    with open(output_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    return output_path\n",
        "\n",
        "def process_tts(segments, temp_dir, voice=\"female_1\"):\n",
        "    \"\"\"TTS ìƒì„±\"\"\"\n",
        "    print(f\"[TTS] ìŒì„± ìƒì„± ì¤‘... (ëª©ì†Œë¦¬: {voice})\")\n",
        "    voice_id = VOICES.get(voice, VOICES[\"female_1\"])\n",
        "    \n",
        "    for i, seg in enumerate(segments):\n",
        "        print(f\"  {i+1}/{len(segments)}\", end=\"\\r\")\n",
        "        tts_path = f\"{temp_dir}/tts_{i:04d}.mp3\"\n",
        "        generate_tts(seg[\"translated\"], tts_path, voice_id)\n",
        "        seg[\"tts_path\"] = tts_path\n",
        "    \n",
        "    print(f\"\\n[TTS] {len(segments)}ê°œ ì™„ë£Œ\")\n",
        "    return segments\n",
        "\n",
        "def mix_audio(original_audio, segments, output_path, original_volume=0.15):\n",
        "    \"\"\"ì˜¤ë””ì˜¤ ë¯¹ì‹±\"\"\"\n",
        "    print(\"[ë¯¹ì‹±] ì˜¤ë””ì˜¤ ë¯¹ì‹± ì¤‘...\")\n",
        "    \n",
        "    inputs = [\"-i\", original_audio]\n",
        "    filter_parts = [f\"[0:a]volume={original_volume}[orig]\"]\n",
        "    overlay_inputs = [\"[orig]\"]\n",
        "    \n",
        "    for i, seg in enumerate(segments):\n",
        "        inputs.extend([\"-i\", seg[\"tts_path\"]])\n",
        "        delay_ms = int(seg[\"start\"] * 1000)\n",
        "        filter_parts.append(f\"[{i+1}:a]adelay={delay_ms}|{delay_ms}[dub{i}]\")\n",
        "        overlay_inputs.append(f\"[dub{i}]\")\n",
        "    \n",
        "    filter_parts.append(f\"{''.join(overlay_inputs)}amix=inputs={len(segments)+1}:duration=longest:normalize=0[out]\")\n",
        "    \n",
        "    cmd = [\"ffmpeg\", \"-y\"] + inputs + [\"-filter_complex\", \";\".join(filter_parts), \"-map\", \"[out]\", \"-c:a\", \"libmp3lame\", output_path]\n",
        "    subprocess.run(cmd, capture_output=True, check=True)\n",
        "    print(\"[ë¯¹ì‹±] ì™„ë£Œ\")\n",
        "    return output_path\n",
        "\n",
        "def replace_audio(video_path, audio_path, output_path):\n",
        "    \"\"\"ì˜ìƒ ì˜¤ë””ì˜¤ êµì²´\"\"\"\n",
        "    cmd = [\"ffmpeg\", \"-y\", \"-i\", video_path, \"-i\", audio_path, \"-c:v\", \"copy\", \"-map\", \"0:v:0\", \"-map\", \"1:a:0\", \"-shortest\", output_path]\n",
        "    subprocess.run(cmd, capture_output=True, check=True)\n",
        "    return output_path\n",
        "\n",
        "def generate_srt(segments, output_path):\n",
        "    \"\"\"SRT ìë§‰ ìƒì„±\"\"\"\n",
        "    def fmt_time(s):\n",
        "        h, m, sec = int(s//3600), int((s%3600)//60), int(s%60)\n",
        "        ms = int((s%1)*1000)\n",
        "        return f\"{h:02d}:{m:02d}:{sec:02d},{ms:03d}\"\n",
        "    \n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for i, seg in enumerate(segments, 1):\n",
        "            f.write(f\"{i}\\n{fmt_time(seg['start'])} --> {fmt_time(seg['end'])}\\n{seg['translated']}\\n\\n\")\n",
        "    return output_path\n",
        "\n",
        "print(\"âœ… í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5ï¸âƒ£ ë”ë¹™ ì‹¤í–‰"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ğŸ¬ ë”ë¹™ ì„¤ì •\n",
        "youtube_url = \"https://www.youtube.com/watch?v=-dtGzB4zqlc\" #@param {type:\"string\"}\n",
        "voice = \"female_1\" #@param [\"female_1\", \"female_2\", \"male_1\", \"male_2\"]\n",
        "tone = \"formal\" #@param [\"formal\", \"casual\", \"narration\"]\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "# ì„ì‹œ í´ë”\n",
        "temp_dir = tempfile.mkdtemp()\n",
        "print(f\"ì„ì‹œ í´ë”: {temp_dir}\")\n",
        "\n",
        "# 1. ìœ íŠœë¸Œ ë‹¤ìš´ë¡œë“œ\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"[1/6] ìœ íŠœë¸Œ ë‹¤ìš´ë¡œë“œ\")\n",
        "video_path = download_youtube(youtube_url, temp_dir)\n",
        "\n",
        "# 2. ì˜¤ë””ì˜¤ ì¶”ì¶œ\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"[2/6] ì˜¤ë””ì˜¤ ì¶”ì¶œ\")\n",
        "audio_wav = f\"{temp_dir}/audio.wav\"\n",
        "audio_mp3 = f\"{temp_dir}/audio.mp3\"\n",
        "extract_audio(video_path, audio_wav)\n",
        "subprocess.run([\"ffmpeg\", \"-y\", \"-i\", audio_wav, \"-q:a\", \"2\", audio_mp3], capture_output=True)\n",
        "\n",
        "# 3. VAD\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"[3/6] VAD ìŒì„± ê°ì§€\")\n",
        "segments = detect_speech_vad(audio_wav)\n",
        "\n",
        "# 4. Whisper\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"[4/6] Whisper í…ìŠ¤íŠ¸ ë³€í™˜\")\n",
        "segments = transcribe_segments(audio_mp3, segments, temp_dir)\n",
        "\n",
        "# 5. ë²ˆì—­\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"[5/6] ë²ˆì—­\")\n",
        "segments = translate_segments(segments, tone)\n",
        "\n",
        "# 6. TTS\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"[6/6] TTS ìƒì„±\")\n",
        "segments = process_tts(segments, temp_dir, voice)\n",
        "\n",
        "# 7. ë¯¹ì‹± & ìµœì¢… ì¶œë ¥\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"[ìµœì¢…] í•©ì„± ì¤‘...\")\n",
        "mixed_audio = f\"{temp_dir}/mixed.mp3\"\n",
        "mix_audio(audio_mp3, segments, mixed_audio)\n",
        "\n",
        "output_video = \"/content/dubbed_output.mp4\"\n",
        "output_srt = \"/content/dubbed_output.srt\"\n",
        "replace_audio(video_path, mixed_audio, output_video)\n",
        "generate_srt(segments, output_srt)\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"\\n\" + \"=\"*50)\n",
        "print(f\"âœ… ì™„ë£Œ! ì†Œìš”ì‹œê°„: {int(elapsed//60)}ë¶„ {int(elapsed%60)}ì´ˆ\")\n",
        "print(f\"ğŸ“¹ ì˜ìƒ: {output_video}\")\n",
        "print(f\"ğŸ“ ìë§‰: {output_srt}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6ï¸âƒ£ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# ì˜ìƒ ë‹¤ìš´ë¡œë“œ\n",
        "files.download('/content/dubbed_output.mp4')\n",
        "files.download('/content/dubbed_output.srt')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“º ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Video\n",
        "Video('/content/dubbed_output.mp4', width=640)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
