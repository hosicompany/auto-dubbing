#!/usr/bin/env python3
"""
Smart Dubbing Pipeline v2
- ì›ë³¸ íƒ€ì´ë°ì— ë§ì¶˜ ìì—°ìŠ¤ëŸ¬ìš´ ë”ë¹™
- ë²ˆì—­ ì‹œ ê¸¸ì´ ê³ ë ¤
- Time-stretchë¡œ ì‹±í¬ ë§ì¶¤
"""

import os
import sys
import json
import subprocess
import tempfile
import argparse
from pathlib import Path
from openai import OpenAI
import requests

# ============ ì„¤ì • ============
ELEVENLABS_API_KEY = os.getenv("ELEVENLABS_API_KEY", "sk_7b0a163f718c23222429625faebe9dabf428825ebc36d6c2")
ELEVENLABS_VOICE_ID = "pFZP5JQG7iQjIQuC4Bku"  # Lily
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Time-stretch í—ˆìš© ë²”ìœ„ (ì˜ˆ: 0.25 = Â±25%)
STRETCH_TOLERANCE = 0.25
MIN_STRETCH_RATIO = 1 - STRETCH_TOLERANCE  # 0.75x
MAX_STRETCH_RATIO = 1 + STRETCH_TOLERANCE  # 1.25x


def extract_audio(video_path: str, output_path: str) -> str:
    """ì˜ìƒì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ (MP3 í˜•ì‹)"""
    # Whisper APIëŠ” mp3, wav, m4a ë“± ì§€ì› - mp3ê°€ ê°€ì¥ í˜¸í™˜ì„± ì¢‹ìŒ
    if output_path.endswith('.wav'):
        output_path = output_path.replace('.wav', '.mp3')
    
    cmd = [
        "ffmpeg", "-y", "-i", video_path,
        "-vn", "-acodec", "libmp3lame", "-ar", "16000", "-ac", "1",
        "-q:a", "2",
        output_path
    ]
    subprocess.run(cmd, capture_output=True, check=True)
    return output_path


def transcribe_with_timestamps(audio_path: str) -> list:
    """Whisperë¡œ íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ ìë§‰ ì¶”ì¶œ"""
    client = OpenAI(api_key=OPENAI_API_KEY)
    
    with open(audio_path, "rb") as f:
        response = client.audio.transcriptions.create(
            model="whisper-1",
            file=f,
            response_format="verbose_json",
            timestamp_granularities=["segment"]
        )
    
    segments = []
    for seg in response.segments:
        segments.append({
            "start": seg.start,
            "end": seg.end,
            "duration": seg.end - seg.start,
            "text": seg.text.strip()
        })
    
    print(f"[*] {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ ì™„ë£Œ")
    return segments


def translate_with_length_hint(segments: list, target_lang: str = "Korean") -> list:
    """ê¸¸ì´ íŒíŠ¸ë¥¼ í¬í•¨í•´ì„œ ë²ˆì—­"""
    client = OpenAI(api_key=OPENAI_API_KEY)
    
    # ë²ˆì—­í•  í…ìŠ¤íŠ¸ì™€ ê¸¸ì´ ì •ë³´ ì¤€ë¹„
    texts_with_duration = []
    for i, seg in enumerate(segments):
        texts_with_duration.append({
            "id": i,
            "text": seg["text"],
            "duration_sec": round(seg["duration"], 1)
        })
    
    prompt = f"""You are a professional dubbing translator. Translate the following segments to {target_lang}.

IMPORTANT RULES:
1. Each segment has a duration in seconds - the translation should be speakable in roughly that time
2. If the direct translation is too long, paraphrase or shorten it naturally
3. If the direct translation is too short, you can slightly expand it
4. Prioritize natural speech over literal translation
5. Keep the meaning and tone intact

Return a JSON array with "id" and "translation" for each segment.

Segments:
{json.dumps(texts_with_duration, ensure_ascii=False, indent=2)}

Return ONLY the JSON array, no other text."""

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3
    )
    
    content = response.choices[0].message.content
    
    # JSON ì¶”ì¶œ (ì½”ë“œë¸”ë¡ ì œê±°)
    if "```json" in content:
        content = content.split("```json")[1].split("```")[0]
    elif "```" in content:
        content = content.split("```")[1].split("```")[0]
    
    content = content.strip()
    result = json.loads(content)
    
    # ë²ˆì—­ ê²°ê³¼ ë§¤í•‘
    translation_map = {item["id"]: item["translation"] for item in result}
    
    for i, seg in enumerate(segments):
        seg["translated"] = translation_map.get(i, seg["text"])
    
    print(f"[*] ë²ˆì—­ ì™„ë£Œ")
    return segments


def generate_tts(text: str, output_path: str) -> float:
    """ElevenLabs TTS ìƒì„± ë° ì‹¤ì œ ê¸¸ì´ ë°˜í™˜"""
    url = f"https://api.elevenlabs.io/v1/text-to-speech/{ELEVENLABS_VOICE_ID}"
    
    headers = {
        "xi-api-key": ELEVENLABS_API_KEY,
        "Content-Type": "application/json"
    }
    
    data = {
        "text": text,
        "model_id": "eleven_multilingual_v2",
        "voice_settings": {
            "stability": 0.5,
            "similarity_boost": 0.75
        }
    }
    
    response = requests.post(url, headers=headers, json=data)
    response.raise_for_status()
    
    with open(output_path, "wb") as f:
        f.write(response.content)
    
    # ffprobeë¡œ ì‹¤ì œ ê¸¸ì´ ì¸¡ì •
    duration = get_audio_duration(output_path)
    return duration


def get_audio_duration(audio_path: str) -> float:
    """ì˜¤ë””ì˜¤ íŒŒì¼ ê¸¸ì´ ì¸¡ì •"""
    cmd = [
        "ffprobe", "-v", "error",
        "-show_entries", "format=duration",
        "-of", "default=noprint_wrappers=1:nokey=1",
        audio_path
    ]
    result = subprocess.run(cmd, capture_output=True, text=True, check=True)
    return float(result.stdout.strip())


def time_stretch_audio(input_path: str, output_path: str, ratio: float) -> str:
    """ì˜¤ë””ì˜¤ ì†ë„ ì¡°ì ˆ (ratio > 1 = ëŠë¦¬ê²Œ, ratio < 1 = ë¹ ë¥´ê²Œ)"""
    # atempoëŠ” 0.5~2.0 ë²”ìœ„ë§Œ ì§€ì›, ê·¸ ì™¸ëŠ” ì²´ì´ë‹ í•„ìš”
    if ratio < 0.5:
        ratio = 0.5
    elif ratio > 2.0:
        ratio = 2.0
    
    cmd = [
        "ffmpeg", "-y", "-i", input_path,
        "-filter:a", f"atempo={ratio}",
        "-vn", output_path
    ]
    subprocess.run(cmd, capture_output=True, check=True)
    return output_path


def process_segments_with_sync(segments: list, temp_dir: str) -> list:
    """ê° ì„¸ê·¸ë¨¼íŠ¸ TTS ìƒì„± + ì‹±í¬ ë§ì¶¤"""
    processed = []
    
    for i, seg in enumerate(segments):
        print(f"  ì²˜ë¦¬ ì¤‘: {i+1}/{len(segments)}", end="\r")
        
        tts_path = os.path.join(temp_dir, f"tts_{i:04d}.mp3")
        stretched_path = os.path.join(temp_dir, f"stretched_{i:04d}.mp3")
        
        # TTS ìƒì„±
        tts_duration = generate_tts(seg["translated"], tts_path)
        
        # íƒ€ê²Ÿ ê¸¸ì´
        target_duration = seg["duration"]
        
        # ë¹„ìœ¨ ê³„ì‚°
        if tts_duration > 0:
            stretch_ratio = target_duration / tts_duration
        else:
            stretch_ratio = 1.0
        
        # í—ˆìš© ë²”ìœ„ ë‚´ì—ì„œ time-stretch
        final_path = tts_path
        applied_stretch = 1.0
        
        if MIN_STRETCH_RATIO <= stretch_ratio <= MAX_STRETCH_RATIO:
            # ë²”ìœ„ ë‚´: time-stretch ì ìš©
            if abs(stretch_ratio - 1.0) > 0.05:  # 5% ì´ìƒ ì°¨ì´ë‚  ë•Œë§Œ
                time_stretch_audio(tts_path, stretched_path, stretch_ratio)
                final_path = stretched_path
                applied_stretch = stretch_ratio
        else:
            # ë²”ìœ„ ë°–: ê·¸ëƒ¥ ìì—°ìŠ¤ëŸ½ê²Œ (ì‹±í¬ í¬ê¸°)
            print(f"\n  [!] ì„¸ê·¸ë¨¼íŠ¸ {i}: ê¸¸ì´ ì°¨ì´ í¼ (TTS: {tts_duration:.1f}s, íƒ€ê²Ÿ: {target_duration:.1f}s)")
        
        processed.append({
            **seg,
            "tts_path": final_path,
            "tts_duration": tts_duration,
            "stretch_ratio": applied_stretch,
            "final_duration": get_audio_duration(final_path) if os.path.exists(final_path) else tts_duration
        })
    
    print(f"\n[*] {len(processed)}ê°œ ì„¸ê·¸ë¨¼íŠ¸ TTS ì²˜ë¦¬ ì™„ë£Œ")
    return processed


def mix_audio(original_audio: str, segments: list, output_path: str, 
              original_volume: float = 0.1, dub_volume: float = 1.0):
    """ì›ë³¸ ì˜¤ë””ì˜¤ì™€ ë”ë¹™ ë¯¹ìŠ¤"""
    
    # ë³µì¡í•œ í•„í„° ì²´ì¸ êµ¬ì„±
    inputs = ["-i", original_audio]
    filter_parts = []
    
    # ê° TTS íŒŒì¼ ì…ë ¥ ì¶”ê°€
    for i, seg in enumerate(segments):
        inputs.extend(["-i", seg["tts_path"]])
    
    # ì›ë³¸ ì˜¤ë””ì˜¤ ë³¼ë¥¨ ì¡°ì ˆ
    filter_parts.append(f"[0:a]volume={original_volume}[orig]")
    
    # ê° ë”ë¹™ ì„¸ê·¸ë¨¼íŠ¸ì— ë”œë ˆì´ ì ìš©
    overlay_inputs = ["[orig]"]
    for i, seg in enumerate(segments):
        delay_ms = int(seg["start"] * 1000)
        filter_parts.append(
            f"[{i+1}:a]volume={dub_volume},adelay={delay_ms}|{delay_ms}[dub{i}]"
        )
        overlay_inputs.append(f"[dub{i}]")
    
    # ëª¨ë“  íŠ¸ë™ ë¯¹ìŠ¤
    filter_parts.append(
        f"{''.join(overlay_inputs)}amix=inputs={len(segments)+1}:duration=longest:normalize=0[out]"
    )
    
    filter_complex = ";".join(filter_parts)
    
    cmd = ["ffmpeg", "-y"] + inputs + [
        "-filter_complex", filter_complex,
        "-map", "[out]",
        "-c:a", "libmp3lame", "-q:a", "2",
        output_path
    ]
    
    subprocess.run(cmd, capture_output=True, check=True)
    return output_path


def replace_video_audio(video_path: str, audio_path: str, output_path: str):
    """ì˜ìƒì˜ ì˜¤ë””ì˜¤ êµì²´"""
    cmd = [
        "ffmpeg", "-y",
        "-i", video_path,
        "-i", audio_path,
        "-c:v", "copy",
        "-map", "0:v:0", "-map", "1:a:0",
        "-shortest",
        output_path
    ]
    subprocess.run(cmd, capture_output=True, check=True)
    return output_path


def main():
    parser = argparse.ArgumentParser(description="Smart Dubbing Pipeline")
    parser.add_argument("input", help="ì…ë ¥ ì˜ìƒ íŒŒì¼")
    parser.add_argument("-o", "--output", help="ì¶œë ¥ íŒŒì¼ëª…")
    parser.add_argument("-l", "--lang", default="Korean", help="íƒ€ê²Ÿ ì–¸ì–´ (ê¸°ë³¸: Korean)")
    parser.add_argument("--original-volume", type=float, default=0.1, help="ì›ë³¸ ì˜¤ë””ì˜¤ ë³¼ë¥¨ (0-1)")
    parser.add_argument("--keep-temp", action="store_true", help="ì„ì‹œ íŒŒì¼ ìœ ì§€")
    args = parser.parse_args()
    
    input_path = Path(args.input)
    if not input_path.exists():
        print(f"[!] íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}")
        sys.exit(1)
    
    output_path = args.output or f"{input_path.stem}_dubbed_ko{input_path.suffix}"
    
    print(f"\n{'='*50}")
    print(f"Smart Dubbing Pipeline v2")
    print(f"{'='*50}")
    print(f"ì…ë ¥: {input_path}")
    print(f"ì¶œë ¥: {output_path}")
    print(f"íƒ€ê²Ÿ ì–¸ì–´: {args.lang}")
    print(f"ì›ë³¸ ë³¼ë¥¨: {args.original_volume}")
    print(f"{'='*50}\n")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        if args.keep_temp:
            temp_dir = "./temp_dub"
            os.makedirs(temp_dir, exist_ok=True)
        
        # 1. ì˜¤ë””ì˜¤ ì¶”ì¶œ
        print("[1/5] ì˜¤ë””ì˜¤ ì¶”ì¶œ ì¤‘...")
        audio_path = os.path.join(temp_dir, "original.mp3")
        extract_audio(str(input_path), audio_path)
        
        # 2. Whisper ìë§‰ ì¶”ì¶œ
        print("[2/5] ìë§‰ ì¶”ì¶œ ì¤‘ (Whisper)...")
        segments = transcribe_with_timestamps(audio_path)
        
        # 3. ê¸¸ì´ ê³ ë ¤ ë²ˆì—­
        print("[3/5] ë²ˆì—­ ì¤‘ (ê¸¸ì´ ìµœì í™”)...")
        segments = translate_with_length_hint(segments, args.lang)
        
        # 4. TTS + ì‹±í¬ ë§ì¶¤
        print("[4/5] TTS ìƒì„± ë° ì‹±í¬ ì¡°ì • ì¤‘...")
        segments = process_segments_with_sync(segments, temp_dir)
        
        # 5. ì˜¤ë””ì˜¤ ë¯¹ìŠ¤
        print("[5/5] ì˜¤ë””ì˜¤ ë¯¹ì‹± ì¤‘...")
        mixed_audio = os.path.join(temp_dir, "mixed.mp3")
        mix_audio(audio_path, segments, mixed_audio, args.original_volume)
        
        # ìµœì¢… ì˜ìƒ ìƒì„±
        print("[*] ìµœì¢… ì˜ìƒ ìƒì„± ì¤‘...")
        replace_video_audio(str(input_path), mixed_audio, output_path)
        
        # ê²°ê³¼ ë¦¬í¬íŠ¸
        print(f"\n{'='*50}")
        print(f"[âœ“] ì™„ë£Œ: {output_path}")
        print(f"{'='*50}")
        
        # ì‹±í¬ í†µê³„
        stretch_applied = sum(1 for s in segments if abs(s["stretch_ratio"] - 1.0) > 0.05)
        out_of_range = sum(1 for s in segments if s["stretch_ratio"] < MIN_STRETCH_RATIO or s["stretch_ratio"] > MAX_STRETCH_RATIO)
        
        print(f"\nğŸ“Š ì‹±í¬ í†µê³„:")
        print(f"  - ì´ ì„¸ê·¸ë¨¼íŠ¸: {len(segments)}")
        print(f"  - Time-stretch ì ìš©: {stretch_applied}")
        print(f"  - ë²”ìœ„ ì´ˆê³¼ (ìì—°ìŠ¤ëŸ½ê²Œ): {out_of_range}")


if __name__ == "__main__":
    main()
